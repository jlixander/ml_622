{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336242b3-ba45-473c-8117-bffc0053c4d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d94e431-0b59-4af4-9806-d51b39e5fa8f",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb6a27-c1a4-43b5-8885-ad3d29b9511b",
   "metadata": {},
   "source": [
    "In this project we review the _Airplaine Passenger Satisfaction_ dataset found on kaggle.com (https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction). This dataset contains survey information for passengers who recently took a flight with an airline company. The survey questions range take into account their entire experience, from their initial booking all the way through their inflight experience. Understanding which customers are satisfied with their service can lead to targeted promotions and marketing that may be more successful than cold marketing.\n",
    "  \n",
    "The dataset is in tabular form, with ~129k oberservations, where each one represents satisfaction responses from a customer. Most numeric values are constrained in a likert scale between 0-5 where 0 represents very unsatisfied and 5 represents highly satisfied. Due to their equally spaced nature, which does not add value to a model, they will be treated as categorical features with ordinal encoding.\n",
    "\n",
    "Furthermore, the dataset is heterogenous with a combination of numeric and categorical values totaling 25 columns.  Some other numeric values are truly numeric such as _flight distance_ which represents a continuous scale. All other categorical values are nominal where no inherent order is specified. Lastly, the target variable is binary, which identifies if the customer was satisfied with the service or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33a32c-2855-4732-a119-42a6025afefc",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3aced-ceab-4f69-bff9-e267efac8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, LabelEncoder#, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy import stats\n",
    "import six\n",
    "from scipy.stats import randint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "seed =40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52062713-cc24-484e-8b12-15892cc36ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46f8a7-ad14-4487-b5fe-742c4f273ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r\"data/archive/test.csv\")\n",
    "df_train = pd.read_csv(r\"data/archive/train.csv\")\n",
    "\n",
    "#Check if all columns are the same\n",
    "df_train.columns == df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18734f27-87f8-47eb-af53-41b9a4947b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat dfs into one (axis=1)\n",
    "df_conc = pd.concat([df_test, df_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554df2a-0e29-4542-adb0-aa00c6a21c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710af7cd-0198-4cab-b58e-47399aac28fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "277209d4-f088-4257-aae9-e80753af00f8",
   "metadata": {},
   "source": [
    "# 2. Check Dataset for Missing Values and Data Types\n",
    "\n",
    "Column 'Arrival Delay in Minutes' seems to have some missing values. 393 values to be exact. Since it only represents a very small fraction of the dataset, these observatiions will be removed. An alternative approach is to use an imputation method. Simple methods can be using the average or mode value, other advanced methods include **regression imputation**. It is crucial to remove null values as many algorithms are unable to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f9145-fa9c-4459-9fb5-7a8958ef4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649b57d-29f8-49d7-ab7f-3001083f8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove observations which have na in feature \"Arrival Delay in Minutes'\n",
    "df_conc = df_conc.dropna(subset=['Arrival Delay in Minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a41a6-ef67-4d7e-9c22-40bf428828b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_conc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bbf01-e5d9-4161-8a9d-ea67f1a9137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate categorical columns from numerical\n",
    "\n",
    "#define numeric columns to be scaled\n",
    "numeric_columns = ['Age',\n",
    "                   'Flight Distance',\n",
    "                   #'Departure Delay in Minutes',\n",
    "                   'Arrival Delay in Minutes'                   \n",
    "                   ]\n",
    "\n",
    "#define ordinal cat columns (should be left as-is)\n",
    "cat_ord_columns = ['Inflight wifi service',\n",
    "                   'Departure/Arrival time convenient', \n",
    "                   'Ease of Online booking',\n",
    "                   'Gate location', \n",
    "                   'Food and drink', \n",
    "                   'Online boarding', \n",
    "                   'Seat comfort',\n",
    "                   'Inflight entertainment', \n",
    "                   'On-board service', \n",
    "                   'Leg room service',\n",
    "                   'Baggage handling', \n",
    "                   'Checkin service', \n",
    "                   'Inflight service',\n",
    "                   'Cleanliness'\n",
    "                     ]\n",
    "\n",
    "#define nominal cat columns to be one-hot-encoded\n",
    "cat_nom_columns = ['Gender',\n",
    "                   'Customer Type',\n",
    "                   'Type of Travel',\n",
    "                   'Class'                   \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62e24c-1744-4588-b1e4-c2d360ec108f",
   "metadata": {},
   "source": [
    "### 2a. Remove Identifier columns\n",
    "Identifier columns do not add any value to algorithms. In fact, numerical values can introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cc967-8f50-4776-81e2-ddb63b0e85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify features to remove\n",
    "cols_to_remove = ['Unnamed: 0', #row identifier - index\n",
    "                 'id' #unique observation identifier\n",
    "                 ]\n",
    "#drop columns\n",
    "df_conc = df_conc.drop(columns=cols_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a190e-5103-4c1f-8ea9-de2267c7beda",
   "metadata": {},
   "source": [
    "### 2b. Check for Collinearity and remove features as needed\n",
    "'Departure Delay in Minutes' and 'Arrival Delay in Minutes'. Due to high collinearity, the former will be removed. This is because a flight can be delayed by a few minutes and still arrive on time if the flight is cleared to fly at faster speeds. Some algorithms like Logistic Regression can be extremely sensitive to serially correlated features resulting in bias. Bias can lead to underfitting of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38f996-d4c7-4ada-b687-5d796a75d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = add_constant(df_conc[numeric_columns]) #remove target variable\n",
    "\n",
    "# Create a DataFrame that will hold the feature names and their VIFs\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e7c48-5850-4ca1-92cd-11c1b2d64f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conc = df_conc.drop(columns=['Departure Delay in Minutes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a4aff-df08-453f-9eac-a78d579f1308",
   "metadata": {},
   "source": [
    "### 2c. Checking Target Variable\n",
    "Some algorithms are also sensitive to class imbalance. The majority class can introduce some bias. Some class imbalance was found. Not extreme, but it will be corrected in section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268147af-ac0a-4a70-a61d-6a8538e0c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify target column cardinality.\n",
    "print(f\"The target values are: {df_conc['satisfaction'].unique()}\\n\")\n",
    "\n",
    "print(f\"The counts of each target is:\\n{df_conc['satisfaction'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8f095-1467-4314-9b00-279625bdc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "df_conc['satisfaction'].value_counts().plot(kind='bar', color=['blue', 'green'])\n",
    "plt.xlabel('Satisfaction Level')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Balance: Satisfaction Levels')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d06f4-6efb-4937-8739-fa79433afc06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2cbb19b-25d9-4a54-b391-de8116741416",
   "metadata": {},
   "source": [
    "### 2d. Encode Target Variable & Nominal Categorical Features\n",
    "To enconde categorical variables we'll employ Pandas' get_dummies(). Encoding is the process of turning values into algorithhm readable characters. These characters are usually numerical. In R, the labels can remain, but the data type would need to be changed to 'factor'. Label encoder works well for the target variable. However, in nominal categorical features where there is high cardinality, it can give an algorithm the impression that there is order. To avoid this, the one-hot-encoding is used to pivot each class into its own binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f5a8a-0b55-4a3e-8037-bfe747110f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#fit encoder\n",
    "df_conc['satisfaction'] = label_encoder.fit_transform(df_conc['satisfaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fefdcc-8aa6-4325-9c3c-6914eee92f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-encode\n",
    "df_conc = pd.get_dummies(df_conc, columns=cat_nom_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b979c9d-382b-413f-981d-ad4aac624299",
   "metadata": {},
   "source": [
    "### 2e. Scale and Transform Numerical Features\n",
    "Depart delay and departure delay have some negative values. They will not be removed as early departure could be pleasant for some customers and unpleasant for others. This sentiment may display in the likert scores.\n",
    "\n",
    "Scaling helps to put numeric values into a limited readable range for the algorithm, whereas transformations are used to normalize the shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04996180-bdd9-4c3d-ab36-ec5a0a4550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "fig1.suptitle('Density Distribution of Original Numeric Columns', y=1.05)\n",
    "\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    ax=axes[i]\n",
    "    df_conc[column].plot(kind='kde', ax=ax)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1693a653-ab3b-46f3-a816-fc2d8db1239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot boxplots\n",
    "fig2, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))\n",
    "fig2.suptitle('Boxplots of Original Numeric Columns', y=1.05)\n",
    "\n",
    "# Plotting the boxplot for each numeric column\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    ax = axes[i]\n",
    "    df_conc.boxplot(column=column, ax=ax)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602cee7-4762-4ba1-bc11-2f7f07b2bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scale Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_conc[numeric_columns] = scaler.fit_transform(df_conc[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692b295-0aec-4422-b4ca-1d1a832ac469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Power Transform numeric values\n",
    "transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "df_conc[numeric_columns] = transformer.fit_transform(df_conc[numeric_columns])\n",
    "df_conc[numeric_columns] = transformer.transform(df_conc[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7a083-51b2-461f-aeab-48eaf1bea38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))\n",
    "fig3.suptitle('Density Distribution of Original Numeric Columns', y=1.05)\n",
    "\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    ax=axes[i]\n",
    "    df_conc[column].plot(kind='kde', ax=ax)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True)\n",
    "\n",
    "#remove empty plots in grid\n",
    "# for j in range(len(numeric_columns), 2*2):\n",
    "#     fig1.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a35fb-6ff6-4e6a-87f3-137d9323519a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5a7a8-66d9-47ee-9b1f-6f72659ec355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "609ab6a6-ec31-4e26-9a05-59fd918e6193",
   "metadata": {},
   "source": [
    "# 3. Fixing Class Imbalance & Splitting Data in Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427a60a-763b-4a49-a095-07d316f9f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Separate Features and Target Variable\n",
    "X = df_conc.drop(['satisfaction'], axis=1)\n",
    "y = df_conc['satisfaction']\n",
    "\n",
    "#Split data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "#Execute SMOTE\n",
    "sm = SMOTE(random_state=seed)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "print(\"Resampled dataset shape %s\" % Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c9afd-1f1e-4c61-9341-925cd7b29eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save preprocessed dataset\n",
    "X_train.to_csv(r\"preprocessed_data/X_train.csv\")\n",
    "y_train.to_csv(r\"preprocessed_data/y_train.csv\")\n",
    "X_test.to_csv(r\"preprocessed_data/X_test.csv\")\n",
    "y_test.to_csv(r\"preprocessed_data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5a6dd-ea37-4365-a3e5-6a7f130f3a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634bdd9-1c6b-4b48-a7fc-f4325b765576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297813c3-08f5-45a4-abf3-e309cf17148d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cb75b-acf8-4173-b3e6-5a77dcade07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e51d42-3558-4f25-ae6a-3be38352386d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71aae3-d957-405e-8353-e698c28a2731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23793264-5f22-462d-863b-f03fbf7929e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2962f-593a-404f-b8f7-8462a7572232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dafc743a-6a32-41e3-a4ae-eb4fff9a474e",
   "metadata": {},
   "source": [
    "# 4. Initialize Scikitlearn Pipeline and Pipeline parameters\n",
    "Pipeline() is a scikitlearn library used to streamline the training process. It allows a datascientist to create a systemized approach to training several machine learning models sequentially. It also allows the integration of other preprocessing steps such as standardizing values or encoding categorical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e90ec7-9ef4-4557-a653-2d58e45fcb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize algorithms\n",
    "pipeline_logistic = Pipeline([\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "pipeline_svm = Pipeline([\n",
    "    ('model', SVC())\n",
    "])\n",
    "pipeline_rf = Pipeline([\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "pipeline_gb = Pipeline([\n",
    "    ('model', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "#initialize parameters search for each algorithm\n",
    "# Define parameter grids for each model\n",
    "param_grid_logistic = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__kernel': ['linear', 'rbf'],\n",
    "    'model__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03c49e-2ac8-46ff-b731-7a37678b2e2a",
   "metadata": {},
   "source": [
    "# 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed3794-3acc-4fa5-99da-77d9adb7e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_logistic = GridSearchCV(pipeline_logistic, param_grid_logistic, cv=5, scoring='accuracy', n_jobs = 4)\n",
    "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='accuracy', n_jobs = 4)\n",
    "grid_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs = 4)\n",
    "grid_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=5, scoring='accuracy', n_jobs = 4)\n",
    "\n",
    "#fit models\n",
    "grid_logistic.fit(X_train, y_train)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "grid_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fd97c-201e-4fd6-ab91-1a815065eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate each model\n",
    "models = {\n",
    "    'Logistic Regression': grid_logistic,\n",
    "    'SVM': grid_svm,\n",
    "    'Random Forest': grid_rf,\n",
    "    'Gradient Boosting': grid_gb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85840d-edde-406e-b95b-f0ee10c712bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Best parameters for {name}: {model.best_params_}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for {name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541c09f-3323-49bc-a5ec-6a9f9bf9a3de",
   "metadata": {},
   "source": [
    "# 6. Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242b819-a7ca-4365-900d-ea98ef8c4245",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7e345-6f0b-4f96-8258-7cf9ff1013bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_preds[name] = model.predict(X_test)\n",
    "    conf_matrices[name] = confusion_matrix(y_test, y_preds[name])\n",
    "\n",
    "# Plot 2x2 grid of confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, cm) in zip(axes, conf_matrices.items()):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "    ax.set_title(f'Confusion Matrix for {name}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353b3ae-8d4f-4ee8-82fd-d3c0763d1014",
   "metadata": {},
   "source": [
    "### 6.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e0cac-84e5-475e-9431-dff4d888edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bfcb2-c861-42d0-a5f9-654c0c8b429b",
   "metadata": {},
   "source": [
    "### 6.3 Best Model Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4b89-d7f0-4d0a-997d-0a83e0ac8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain best model:\n",
    "# knn_model.fit(X_train, y_train)\n",
    "# y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Compute performance metrics for best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "f1 = report['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf055b-f1c3-4786-b352-95c1c3436c1b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd93886-0a76-4c54-9150-d3385f9c0073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
